{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIREMENTS + UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.read_data import read_data_sequences\n",
    "from scripts.seq_preprocess import tf_idf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train, sequences_test, proteins_test, y_train = read_data_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = tf_idf(sequences_train, sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'V': 0, 'S': 1, 'Y': 2, 'F': 3, 'A': 4, 'W': 5, 'K': 6, 'I': 7, 'N': 8, 'C': 9, 'L': 10, 'R': 11, 'T': 12, 'X': 13, 'M': 14, 'G': 15, 'E': 16, 'D': 17, 'Q': 18, 'P': 19, 'H': 20}\n"
     ]
    }
   ],
   "source": [
    "amino_acids = list(set(\"\".join(sequences_train)))\n",
    "aa_dict = {aa:i for i, aa in enumerate(amino_acids)}\n",
    "print(aa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (4399,) (4399,)\n",
      "Validation: (489,) (489,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(np.array(sequences_train), np.array(y_train), test_size=0.1)\n",
    "print ('Training:', train_x.shape, train_y.shape)\n",
    "print ('Validation:', val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "class ProteinSeqdataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 proteins_seq,\n",
    "                 y= None, \n",
    "                 vocab = None,\n",
    "                 padding_mark = \"PAD\"):\n",
    "\n",
    "        self.y = y\n",
    "        self.padding_mark = padding_mark\n",
    "        self.proteins_seq = proteins_seq      \n",
    "        #Maximum length of sequences\n",
    "        self.max_len_sequence= max([len(proteins_seq[i]) for i in range(len(proteins_seq))])\n",
    "\n",
    "        #length of each sequence\n",
    "        self.lengths = torch.Tensor([len(prot) for prot in self.proteins_seq])\n",
    "\n",
    "        # Allow to import a vocabulary (for valid/test datasets, that will use the training vocabulary)\n",
    "        if vocab is not None:\n",
    "            self.aa2id, self.id2aa = vocab\n",
    "        else:\n",
    "            # If no vocabulary imported, build it (and reverse)\n",
    "            self.aa2id, self.id2aa = self.build_vocab()\n",
    "        self.vocab_size = len(self.aa2id)\n",
    "        #print(self.aa2id)\n",
    "        #print(self.id2aa)\n",
    "        #Convert to Tensor and apply the vocabulary\n",
    "        sequences_tensor = list(map(lambda prot: torch.Tensor([self.aa2id[aa] for aa in prot]),self.proteins_seq))\n",
    "        #Pad the sequence\n",
    "        sequences_padded = pad_sequence(sequences_tensor,batch_first = True, padding_value =0)\n",
    "\n",
    "        #One Hot encoding\n",
    "        sequences_encoded = one_hot(sequences_padded.to(torch.int64),num_classes=self.vocab_size)\n",
    "\n",
    "        #Convert to Torch \n",
    "        self.X = sequences_encoded\n",
    "        self.y = torch.Tensor(self.y)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.X,self.y\n",
    "    \n",
    "    def get_lengths(self):\n",
    "        return self.lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proteins_seq)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The iterator just gets one particular example with its category\n",
    "        # The dataloader will take care of the shuffling and batching\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.X[idx], self.y[idx], self.lengths[idx]\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"\n",
    "          Function to build the Amino Acids vocabulary\n",
    "          \n",
    "          Returns\n",
    "          ----------------------------\n",
    "            aa2id : <dict{str:int}> \n",
    "                Dictionary to go from an Amino Acids to an id\n",
    "            id2aa : <dict{int:str}> \n",
    "                Dictionary to go from an id to an Amino Acids\n",
    "        \"\"\"\n",
    "        #Set of the Amino Acids\n",
    "        amino_acids = list(set(\"\".join(self.proteins_seq)))\n",
    "        #print(amino_acids)\n",
    "        #Build Vocab (+1 for the padding)\n",
    "        aa2id = {aa:i+1 for i, aa in enumerate(amino_acids)}\n",
    "        id2aa = {i+1:aa for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "        #Add the Padding id \n",
    "        aa2id = {\"PAD\":self.padding_mark,**aa2id}\n",
    "        id2aa = {**id2aa,self.padding_mark:\"PAD\"}\n",
    "\n",
    "        return aa2id, id2aa\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        # A simple way to get the training vocab when building the valid/test \n",
    "        return self.aa2id, self.id2aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinSeqdataset(train_x,train_y)\n",
    "val_dataset = ProteinSeqdataset(val_x,val_y)\n",
    "#X,y = train_dataset.get_data()\n",
    "aa2id,id2aa = train_dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 96., 369., 183.,  ...,  81., 779., 836.])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = train_dataset.get_lengths()\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_class = len(set(y_train))\n",
    "nb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(aa2id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(train_dataset, batch_size = 200, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ProteinLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, vocab_size, embedding_dim):\n",
    "        super(ProteinLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        print(x.shape)\n",
    "        out, _ = self.lstm(x)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = ProteinLSTM(hidden_size=64, num_layers=2, num_classes=nb_class, vocab_size=vocab_size, embedding_dim=64)\n",
    "# Create an optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
    "# The criterion is a binary cross entropy loss based on logits - meaning that the sigmoid is integrated into the criterion\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def train_epoch(model, opt, criterion, dataloader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i, (x, y, len_) in enumerate(dataloader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        len_ = len_.to(device)\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        pred = model.forward(x, len_)\n",
    "        # (2) Compute the loss \n",
    "        loss = criterion(pred,y)\n",
    "        # (3) Compute gradients with the criterion\n",
    "        loss.backward()\n",
    "        # (4) Update weights with the optimizer\n",
    "        opt.step()    \n",
    "        losses.append(loss.item())\n",
    "        # Count the number of correct predictions in the batch - here, you'll need to use the sigmoid\n",
    "        num_corrects = (torch.round(torch.sigmoid(pred)) == y).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(y)\n",
    "        \n",
    "        if (i%20 == 0):\n",
    "            print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training acc = \" + str(acc.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the evaluation ! We don't need the optimizer here. \n",
    "def eval_model(model, criterion, evalloader):\n",
    "    model.eval()\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, len_) in enumerate(evalloader):\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            len_ = len_.to(device)\n",
    "            pred =  model.forward(x, len_)\n",
    "            loss = criterion(pred,y)\n",
    "            num_corrects =(torch.round(torch.sigmoid(pred)) == y).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(y)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/(i+1), total_epoch_acc/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which will help you execute experiments rapidly - with a early_stopping option when necessary. \n",
    "def experiment(model, opt, criterion, num_epochs = 5, early_stopping = True):\n",
    "    train_losses = []\n",
    "    if early_stopping: \n",
    "        best_valid_loss = 10. \n",
    "    print(\"Beginning training...\")\n",
    "    for e in range(num_epochs):\n",
    "        print(\"Epoch \" + str(e+1) + \":\")\n",
    "        train_losses += train_epoch(model, opt, criterion, training_dataloader)\n",
    "        valid_loss, valid_acc = eval_model(model, criterion, valid_dataloader)\n",
    "        print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(valid_loss) + \"; Validation acc = \" + str(valid_acc))\n",
    "        if early_stopping:\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "            else:\n",
    "                print(\"Early stopping.\")\n",
    "                break  \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses \u001b[39m=\u001b[39m experiment(model, opt, criterion,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[235], line 9\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(model, opt, criterion, num_epochs, early_stopping)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     train_losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_epoch(model, opt, criterion, training_dataloader)\n\u001b[0;32m     10\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m eval_model(model, criterion, valid_dataloader)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m : Validation loss = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valid_loss) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m; Validation acc = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valid_acc))\n",
      "Cell \u001b[1;32mIn[233], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, opt, criterion, dataloader)\u001b[0m\n\u001b[0;32m      8\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m \u001b[39m# (1) Forward\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x, len_)\n\u001b[0;32m     11\u001b[0m \u001b[39m# (2) Compute the loss \u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred,y)\n",
      "Cell \u001b[1;32mIn[196], line 17\u001b[0m, in \u001b[0;36mProteinLSTM.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(x, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[0;32m     18\u001b[0m out, _ \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_packed_sequence(out, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:697\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    696\u001b[0m                        ):\n\u001b[1;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    699\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    701\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:206\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    204\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[1;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 3"
     ]
    }
   ],
   "source": [
    "train_losses = experiment(model, opt, criterion,num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "945fb5a7f3f5c89583143137abd22c2112b7db8dd1e354ffad5b7af59f8c26a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
