{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIREMENTS + UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.read_data import read_data_sequences\n",
    "from scripts.seq_preprocess import tf_idf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train, sequences_test, proteins_test, y_train = read_data_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = tf_idf(sequences_train, sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': 0, 'M': 1, 'V': 2, 'W': 3, 'X': 4, 'T': 5, 'D': 6, 'G': 7, 'C': 8, 'I': 9, 'Y': 10, 'A': 11, 'Q': 12, 'N': 13, 'F': 14, 'S': 15, 'R': 16, 'H': 17, 'P': 18, 'L': 19, 'K': 20}\n"
     ]
    }
   ],
   "source": [
    "amino_acids = list(set(\"\".join(sequences_train)))\n",
    "aa_dict = {aa:i for i, aa in enumerate(amino_acids)}\n",
    "print(aa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (4399,) (4399,)\n",
      "Validation: (489,) (489,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(np.array(sequences_train), np.array(y_train), test_size=0.1)\n",
    "print ('Training:', train_x.shape, train_y.shape)\n",
    "print ('Validation:', val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.seq_dataset import ProteinSeqdataset\n",
    "train_dataset = ProteinSeqdataset(train_x,train_y)\n",
    "val_dataset = ProteinSeqdataset(val_x,val_y)\n",
    "#X,y = train_dataset.get_data()\n",
    "aa2id,id2aa = train_dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21,  9, 16,  4, 12, 16, 11,  2,  6, 14, 16, 19,  6, 20, 10,  3,  2, 10,\n",
       "         8, 20, 19, 12, 17,  8, 21,  6, 11,  3, 16, 21, 21, 20,  6, 17, 11, 20,\n",
       "        14,  4, 10,  8,  3, 19,  6, 21,  3, 15, 14, 20,  8,  3, 11, 17, 17, 13,\n",
       "        12,  3, 21, 16, 11, 21, 16, 11,  7, 15, 15, 17, 18,  7, 14,  1,  1, 12,\n",
       "         2, 21, 10, 17, 21, 13,  9, 12, 20,  3, 12, 20, 21,  7,  3, 21, 12, 11,\n",
       "        20,  6,  1,  1, 16,  8, 13, 10, 12,  3, 15,  7, 12,  6, 14,  6,  6, 17,\n",
       "         1, 17, 17,  7, 20, 10, 20, 14, 15, 12,  1,  1, 14, 16, 15, 21,  3, 15,\n",
       "        15,  3,  1, 16,  3,  9,  7,  7, 19,  7,  3, 10, 12, 12, 14, 10, 20,  1,\n",
       "         3, 21,  3, 16, 16, 19,  7, 11, 19,  1, 17, 14, 17,  1, 14,  3,  2,  7,\n",
       "         7, 15, 20, 21, 17, 10,  1,  9, 11, 21,  3,  6, 11, 13, 19, 20,  7, 19,\n",
       "         7, 16, 18,  7, 21,  7, 20, 16, 15, 10, 21,  3, 10, 14,  3,  8, 13, 17,\n",
       "        15, 20,  3, 14, 21,  3, 13,  7, 11, 10, 13, 16, 21, 10,  3, 11, 11, 20,\n",
       "         2, 14, 10, 18,  3, 18, 19, 17,  6, 10, 11, 20,  9, 17,  8,  1, 16,  1,\n",
       "        15, 14, 20, 20,  8, 21, 10,  8,  8,  7, 16,  8, 20, 16,  3, 17,  8, 21,\n",
       "        13, 15, 12, 13, 12, 20, 17, 21, 15, 20,  1,  1, 13,  1, 10, 12,  7, 20,\n",
       "        21,  3,  4,  6, 16, 13, 20, 21, 17,  6, 10, 13,  6, 12,  1, 16, 20,  8,\n",
       "         3,  6, 11,  1, 13,  4, 21, 10, 20, 14,  1, 10,  7, 12,  8,  3,  9,  1,\n",
       "         1,  2,  6, 11, 12,  1, 10, 13,  1, 13, 11, 19,  7,  1, 15, 12, 20, 17,\n",
       "         7,  1,  1, 21, 11, 20, 11, 17, 11, 19,  8,  8,  1, 16, 11, 13,  7, 20,\n",
       "         3, 13, 17, 20,  1, 19,  3, 10,  2,  1, 20,  1, 17, 13,  8, 14,  3, 20,\n",
       "         3, 10, 16, 18, 13, 12,  3,  2, 17,  9, 20, 20, 12, 11, 15, 20,  7, 21,\n",
       "         8, 12,  7,  1, 20, 19, 11, 20, 17,  9, 19, 20, 18,  6, 10, 15, 21, 20,\n",
       "         6, 19,  3, 12, 11,  8,  9, 21,  3,  1,  6, 10, 21, 20, 14,  3,  1, 12,\n",
       "         3, 14,  6, 18, 17,  7, 21, 19,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = train_dataset.get_data()\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([330., 194., 121.,  ..., 263., 116., 971.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = train_dataset.get_lengths()\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_class = len(set(y_train))\n",
    "nb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(aa2id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "training_dataloader = DataLoader(train_dataset, batch_size = 200, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ProteinLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, vocab_size, embedding_dim, lstm_dropout):\n",
    "        super(ProteinLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(lstm_dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embedded_sequence = self.dropout(self.embedding(inputs)) # Inputs:[batch_size,len of sequences]\n",
    "        embedded_sequence = embedded_sequence.permute(1,0,2) #Output: [len of sequences, batch_size, hidden_dim] \n",
    "        lstm_output,(hidden_state,cell_state) = self.lstm(embedded_sequence) \n",
    "        #take the last hidden state for each element in the sequence + Dropout \n",
    "        fc_input = self.dropout(hidden_state[0,:,:]) # [batch_size, len of sequences, hidden state] -> [batch size, last hidden state]\n",
    "        out = self.linear(fc_input) # [batch size, hidden_dim] -> [batch_size, 1 (pred)]\n",
    "        return out.ravel() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'read_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseq_train_eval\u001b[39;00m \u001b[39mimport\u001b[39;00m experiment\n\u001b[0;32m      2\u001b[0m \u001b[39m# Instantiate the model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m ProteinLSTM(\n\u001b[0;32m      4\u001b[0m     hidden_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[0;32m      5\u001b[0m     num_layers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     lstm_dropout\u001b[39m=\u001b[39m\u001b[39m.4\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\scripts\\seq_train_eval.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mread_data\u001b[39;00m \u001b[39mimport\u001b[39;00m read_data_sequences\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwrite_data\u001b[39;00m \u001b[39mimport\u001b[39;00m write_sub\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mseq_dataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ProteinSeqdataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'read_data'"
     ]
    }
   ],
   "source": [
    "from scripts.seq_train_eval import experiment\n",
    "# Instantiate the model\n",
    "model = ProteinLSTM(\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    num_classes=nb_class,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=64,\n",
    "    lstm_dropout=.4\n",
    ")\n",
    "# Create an optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
    "# The criterion is a binary cross entropy loss based on logits - meaning that the sigmoid is integrated into the criterion\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_losses = experiment(\n",
    "    model, training_dataloader, valid_dataloader, opt, criterion, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = ProteinLSTM(hidden_size=64, num_layers=2, num_classes=nb_class, vocab_size=vocab_size, embedding_dim=64)\n",
    "# Create an optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
    "# The criterion is a binary cross entropy loss based on logits - meaning that the sigmoid is integrated into the criterion\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def train_epoch(model, opt, criterion, dataloader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i, (x, y, len_) in enumerate(dataloader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        len_ = len_.to(device)\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        pred = model.forward(x, len_)\n",
    "        # (2) Compute the loss \n",
    "        loss = criterion(pred,y)\n",
    "        # (3) Compute gradients with the criterion\n",
    "        loss.backward()\n",
    "        # (4) Update weights with the optimizer\n",
    "        opt.step()    \n",
    "        losses.append(loss.item())\n",
    "        # Count the number of correct predictions in the batch - here, you'll need to use the sigmoid\n",
    "        num_corrects = (torch.round(torch.sigmoid(pred)) == y).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(y)\n",
    "        \n",
    "        if (i%20 == 0):\n",
    "            print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training acc = \" + str(acc.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the evaluation ! We don't need the optimizer here. \n",
    "def eval_model(model, criterion, evalloader):\n",
    "    model.eval()\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, len_) in enumerate(evalloader):\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            len_ = len_.to(device)\n",
    "            pred =  model.forward(x, len_)\n",
    "            loss = criterion(pred,y)\n",
    "            num_corrects =(torch.round(torch.sigmoid(pred)) == y).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(y)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/(i+1), total_epoch_acc/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which will help you execute experiments rapidly - with a early_stopping option when necessary. \n",
    "def experiment(model, opt, criterion, num_epochs = 5, early_stopping = True):\n",
    "    train_losses = []\n",
    "    if early_stopping: \n",
    "        best_valid_loss = 10. \n",
    "    print(\"Beginning training...\")\n",
    "    for e in range(num_epochs):\n",
    "        print(\"Epoch \" + str(e+1) + \":\")\n",
    "        train_losses += train_epoch(model, opt, criterion, training_dataloader)\n",
    "        valid_loss, valid_acc = eval_model(model, criterion, valid_dataloader)\n",
    "        print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(valid_loss) + \"; Validation acc = \" + str(valid_acc))\n",
    "        if early_stopping:\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "            else:\n",
    "                print(\"Early stopping.\")\n",
    "                break  \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses \u001b[39m=\u001b[39m experiment(model, opt, criterion,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[235], line 9\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(model, opt, criterion, num_epochs, early_stopping)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     train_losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_epoch(model, opt, criterion, training_dataloader)\n\u001b[0;32m     10\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m eval_model(model, criterion, valid_dataloader)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m : Validation loss = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valid_loss) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m; Validation acc = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valid_acc))\n",
      "Cell \u001b[1;32mIn[233], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, opt, criterion, dataloader)\u001b[0m\n\u001b[0;32m      8\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m \u001b[39m# (1) Forward\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x, len_)\n\u001b[0;32m     11\u001b[0m \u001b[39m# (2) Compute the loss \u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred,y)\n",
      "Cell \u001b[1;32mIn[196], line 17\u001b[0m, in \u001b[0;36mProteinLSTM.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(x, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[0;32m     18\u001b[0m out, _ \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_packed_sequence(out, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:697\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    696\u001b[0m                        ):\n\u001b[1;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    699\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    701\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:206\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    204\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[1;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 3"
     ]
    }
   ],
   "source": [
    "train_losses = experiment(model, opt, criterion,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read sequences\n",
    "sequences = list()\n",
    "with open(\"data/sequences.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open(\"data/graph_labels.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        t = line.split(\",\")\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "\n",
    "# Map sequences to\n",
    "vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1, 3))\n",
    "X_train = vec.fit_transform(sequences_train)\n",
    "X_test = vec.transform(sequences_test)\n",
    "\n",
    "# Train a logistic regression classifier and use the classifier to\n",
    "# make predictions\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba_seq = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>class10</th>\n",
       "      <th>class11</th>\n",
       "      <th>class12</th>\n",
       "      <th>class13</th>\n",
       "      <th>class14</th>\n",
       "      <th>class15</th>\n",
       "      <th>class16</th>\n",
       "      <th>class17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11as</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.010125</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.110409</td>\n",
       "      <td>0.040535</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.055511</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.044630</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.082652</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.009913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16pk</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.417447</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.013803</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.063944</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>0.081467</td>\n",
       "      <td>0.069259</td>\n",
       "      <td>0.015457</td>\n",
       "      <td>0.007792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19hc</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.224616</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.369082</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.122394</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>0.044070</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ag9</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>0.456720</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.097420</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.097511</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.095374</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>0.007580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1agx</td>\n",
       "      <td>0.104038</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>0.274372</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.089925</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>0.140387</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.034096</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.174117</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.007112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name    class0    class1    class2    class3    class4    class5    class6  \\\n",
       "0  11as  0.013421  0.010125  0.466346  0.009600  0.015560  0.110409  0.040535   \n",
       "1  16pk  0.021742  0.016125  0.417447  0.009342  0.013803  0.056400  0.063944   \n",
       "2  19hc  0.058075  0.012037  0.224616  0.004999  0.011772  0.043618  0.369082   \n",
       "3  1ag9  0.017705  0.009405  0.456720  0.010811  0.011597  0.097420  0.034768   \n",
       "4  1agx  0.104038  0.013102  0.274372  0.023038  0.014659  0.019519  0.089925   \n",
       "\n",
       "     class7    class8    class9   class10   class11   class12   class13  \\\n",
       "0  0.015619  0.055511  0.010177  0.009209  0.044630  0.006132  0.005926   \n",
       "1  0.019685  0.088272  0.010683  0.008819  0.081549  0.008722  0.009492   \n",
       "2  0.011394  0.122394  0.008381  0.006151  0.020255  0.005745  0.009169   \n",
       "3  0.012282  0.097511  0.013075  0.012560  0.034495  0.007457  0.008537   \n",
       "4  0.012469  0.140387  0.014300  0.006727  0.034096  0.006613  0.019848   \n",
       "\n",
       "    class14   class15   class16   class17  \n",
       "0  0.082652  0.076744  0.017490  0.009913  \n",
       "1  0.081467  0.069259  0.015457  0.007792  \n",
       "2  0.030656  0.044070  0.012128  0.005458  \n",
       "3  0.095374  0.054399  0.018305  0.007580  \n",
       "4  0.174117  0.032383  0.013295  0.007112  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "seq = pd.read_csv(\"sample_submission.csv\")\n",
    "seq.head()\n",
    "#np.sum(seq,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = pd.read_csv(\"sample_submission.csv\")\n",
    "struct.head()\n",
    "#np.sum(struct,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [       nan, 0.01342103, 0.01012509, ..., 0.07674363, 0.0174905 ,\n",
       "        0.00991321],\n",
       "       [       nan, 0.02174239, 0.01612491, ..., 0.06925897, 0.01545664,\n",
       "        0.00779209],\n",
       "       ...,\n",
       "       [       nan, 0.02276303, 0.01242107, ..., 0.04877692, 0.01632783,\n",
       "        0.00632687],\n",
       "       [       nan, 0.02810728, 0.01194174, ..., 0.04407273, 0.01484368,\n",
       "        0.02052648],\n",
       "       [       nan, 0.13940033, 0.00669199, ..., 0.01033002, 0.00609641,\n",
       "        0.00639728]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data_seq = genfromtxt('sample_submission.csv', delimiter=',')\n",
    "my_data_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_seq,y_pred_proba_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "seq_res = pd.read_csv(\"sample_submission.csv\",delimiter= \",\")\n",
    "struct_res = pd.read_csv(\"sample_submission_struct.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>class10</th>\n",
       "      <th>class11</th>\n",
       "      <th>class12</th>\n",
       "      <th>class13</th>\n",
       "      <th>class14</th>\n",
       "      <th>class15</th>\n",
       "      <th>class16</th>\n",
       "      <th>class17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11as</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.010125</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.110409</td>\n",
       "      <td>0.040535</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.055511</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.044630</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.082652</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.009913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16pk</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.417447</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.013803</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.063944</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>0.081467</td>\n",
       "      <td>0.069259</td>\n",
       "      <td>0.015457</td>\n",
       "      <td>0.007792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19hc</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.224616</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.369082</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.122394</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>0.044070</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ag9</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>0.456720</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.097420</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.097511</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.095374</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>0.007580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1agx</td>\n",
       "      <td>0.104038</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>0.274372</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.089925</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>0.140387</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.034096</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.174117</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.007112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>6o81</td>\n",
       "      <td>0.061883</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.436305</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>0.079179</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>0.015116</td>\n",
       "      <td>0.022340</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>0.075634</td>\n",
       "      <td>0.007252</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.106479</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>0.008830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>6o93</td>\n",
       "      <td>0.090354</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.160371</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.180979</td>\n",
       "      <td>0.055051</td>\n",
       "      <td>0.022332</td>\n",
       "      <td>0.233293</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.037230</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>0.076882</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.015508</td>\n",
       "      <td>0.009706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>6ogd</td>\n",
       "      <td>0.022763</td>\n",
       "      <td>0.012421</td>\n",
       "      <td>0.241626</td>\n",
       "      <td>0.013899</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.017378</td>\n",
       "      <td>0.371745</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.091575</td>\n",
       "      <td>0.048777</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.006327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>6okk</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.011942</td>\n",
       "      <td>0.059683</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.028020</td>\n",
       "      <td>0.080133</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.161717</td>\n",
       "      <td>0.012803</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>0.324253</td>\n",
       "      <td>0.026413</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.076939</td>\n",
       "      <td>0.044073</td>\n",
       "      <td>0.014844</td>\n",
       "      <td>0.020526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>6qmn</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.006692</td>\n",
       "      <td>0.016076</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.016734</td>\n",
       "      <td>0.078202</td>\n",
       "      <td>0.031660</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>0.612841</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>0.014229</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.015961</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>0.006397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1223 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      name    class0    class1    class2    class3    class4    class5  \\\n",
       "0     11as  0.013421  0.010125  0.466346  0.009600  0.015560  0.110409   \n",
       "1     16pk  0.021742  0.016125  0.417447  0.009342  0.013803  0.056400   \n",
       "2     19hc  0.058075  0.012037  0.224616  0.004999  0.011772  0.043618   \n",
       "3     1ag9  0.017705  0.009405  0.456720  0.010811  0.011597  0.097420   \n",
       "4     1agx  0.104038  0.013102  0.274372  0.023038  0.014659  0.019519   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1218  6o81  0.061883  0.013655  0.436305  0.005872  0.018467  0.079179   \n",
       "1219  6o93  0.090354  0.011040  0.160371  0.013107  0.027886  0.180979   \n",
       "1220  6ogd  0.022763  0.012421  0.241626  0.013899  0.012017  0.042000   \n",
       "1221  6okk  0.028107  0.011942  0.059683  0.020317  0.028020  0.080133   \n",
       "1222  6qmn  0.139400  0.006692  0.016076  0.008542  0.016734  0.078202   \n",
       "\n",
       "        class6    class7    class8    class9   class10   class11   class12  \\\n",
       "0     0.040535  0.015619  0.055511  0.010177  0.009209  0.044630  0.006132   \n",
       "1     0.063944  0.019685  0.088272  0.010683  0.008819  0.081549  0.008722   \n",
       "2     0.369082  0.011394  0.122394  0.008381  0.006151  0.020255  0.005745   \n",
       "3     0.034768  0.012282  0.097511  0.013075  0.012560  0.034495  0.007457   \n",
       "4     0.089925  0.012469  0.140387  0.014300  0.006727  0.034096  0.006613   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1218  0.013444  0.015116  0.022340  0.009787  0.006971  0.075634  0.007252   \n",
       "1219  0.055051  0.022332  0.233293  0.011121  0.011052  0.037230  0.008301   \n",
       "1220  0.055565  0.017378  0.371745  0.010099  0.009096  0.014772  0.004268   \n",
       "1221  0.030134  0.026982  0.161717  0.012803  0.014893  0.324253  0.026413   \n",
       "1222  0.031660  0.008543  0.612841  0.006668  0.006296  0.014229  0.006298   \n",
       "\n",
       "       class13   class14   class15   class16   class17  \n",
       "0     0.005926  0.082652  0.076744  0.017490  0.009913  \n",
       "1     0.009492  0.081467  0.069259  0.015457  0.007792  \n",
       "2     0.009169  0.030656  0.044070  0.012128  0.005458  \n",
       "3     0.008537  0.095374  0.054399  0.018305  0.007580  \n",
       "4     0.019848  0.174117  0.032383  0.013295  0.007112  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "1218  0.005836  0.106479  0.103218  0.009734  0.008830  \n",
       "1219  0.007795  0.076882  0.027992  0.015508  0.009706  \n",
       "1220  0.009343  0.091575  0.048777  0.016328  0.006327  \n",
       "1221  0.018223  0.076939  0.044073  0.014844  0.020526  \n",
       "1222  0.015961  0.009034  0.010330  0.006096  0.006397  \n",
       "\n",
       "[1223 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.write_data import write_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "res = struct_res.iloc[:,1:]*0.4+seq_res.iloc[:,1:]*0.6\n",
    "write_sub(np.array(res),\"submissions_avg.csv\")\n",
    "#res['name'] = struct_res['name']\n",
    "#res.to_csv(\"submissions_avg.csv\",sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 81.0/81.0 [00:00<00:00, 5.56kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 29.7kB/s]\n",
      "Downloading: 100%|██████████| 86.0/86.0 [00:00<00:00, 19.2kB/s]\n",
      "Downloading: 100%|██████████| 361/361 [00:00<00:00, 26.1kB/s]\n",
      "Downloading: 100%|██████████| 1.68G/1.68G [02:44<00:00, 10.3MB/s]\n",
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "sequence_Example = \"A E T C Z A O\"\n",
    "sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0454,  0.1140, -0.0117,  ..., -0.0875, -0.1143,  0.0204],\n",
       "         [ 0.0923,  0.1391, -0.0524,  ..., -0.1395, -0.0428,  0.0743],\n",
       "         [ 0.1151,  0.0200, -0.0863,  ..., -0.0095, -0.1873,  0.1317],\n",
       "         ...,\n",
       "         [ 0.1079,  0.0977, -0.0583,  ..., -0.1277, -0.0649,  0.1289],\n",
       "         [ 0.0546,  0.0364, -0.0782,  ..., -0.0302, -0.0602,  0.0890],\n",
       "         [ 0.0515,  0.0571, -0.0693,  ..., -0.0394, -0.0663,  0.0977]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2487,  0.2626, -0.2367,  ...,  0.2503,  0.2339, -0.2556]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading:   1%|          | 133M/11.3G [00:05<11:12, 16.6MB/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mRostlab/prot_t5_xl_uniref50\u001b[39m\u001b[39m'\u001b[39m, do_lower_case\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m T5Model\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mRostlab/prot_t5_xl_uniref50\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m sequences_Example \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mA E T C Z A O\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mS K T Z P\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m sequences_Example \u001b[39m=\u001b[39m [re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[UZOB]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, sequence) \u001b[39mfor\u001b[39;00m sequence \u001b[39min\u001b[39;00m sequences_Example]\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\transformers\\modeling_utils.py:2137\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2123\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2125\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2126\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2135\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2136\u001b[0m     )\n\u001b[1;32m-> 2137\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2139\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2140\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2142\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[0;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m    122\u001b[0m     )\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1242\u001b[0m     http_get(\n\u001b[0;32m   1243\u001b[0m         url_to_download,\n\u001b[0;32m   1244\u001b[0m         temp_file,\n\u001b[0;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[0;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[0;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\huggingface_hub\\file_download.py:495\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    488\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    494\u001b[0m )\n\u001b[1;32m--> 495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    497\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\urllib3\\response.py:525\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m     chunk_amt \u001b[39m=\u001b[39m max_chunk_amt\n\u001b[1;32m--> 525\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(chunk_amt)\n\u001b[0;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    527\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:458\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 458\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:502\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    497\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    499\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    504\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   1%|          | 133M/11.3G [00:20<11:12, 16.6MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import re\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', do_lower_case=False)\n",
    "\n",
    "model = T5Model.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "\n",
    "sequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]\n",
    "\n",
    "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]\n",
    "\n",
    "ids = tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids'])\n",
    "attention_mask = torch.tensor(ids['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=input_ids,attention_mask=attention_mask,decoder_input_ids=None)\n",
    "\n",
    "# For feature extraction we recommend to use the encoder embedding\n",
    "encoder_embedding = embedding[2].cpu().numpy()\n",
    "decoder_embedding = embedding[0].cpu().numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "945fb5a7f3f5c89583143137abd22c2112b7db8dd1e354ffad5b7af59f8c26a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
