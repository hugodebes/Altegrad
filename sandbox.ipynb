{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIREMENTS + UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.read_data import read_data_sequences\n",
    "from scripts.seq_preprocess import tf_idf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train, sequences_test, proteins_test, y_train = read_data_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = tf_idf(sequences_train, sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'V': 0, 'S': 1, 'Y': 2, 'F': 3, 'A': 4, 'W': 5, 'K': 6, 'I': 7, 'N': 8, 'C': 9, 'L': 10, 'R': 11, 'T': 12, 'X': 13, 'M': 14, 'G': 15, 'E': 16, 'D': 17, 'Q': 18, 'P': 19, 'H': 20}\n"
     ]
    }
   ],
   "source": [
    "amino_acids = list(set(\"\".join(sequences_train)))\n",
    "aa_dict = {aa:i for i, aa in enumerate(amino_acids)}\n",
    "print(aa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (4399,) (4399,)\n",
      "Validation: (489,) (489,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, val_x, train_y, val_y = train_test_split(np.array(sequences_train), np.array(y_train), test_size=0.1)\n",
    "print ('Training:', train_x.shape, train_y.shape)\n",
    "print ('Validation:', val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import one_hot\n",
    "class ProteinSeqdataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 proteins_seq,\n",
    "                 y= None, \n",
    "                 vocab = None,\n",
    "                 padding_mark = \"PAD\"):\n",
    "\n",
    "        self.y = y\n",
    "        self.padding_mark = padding_mark\n",
    "        self.proteins_seq = proteins_seq      \n",
    "        #Maximum length of sequences\n",
    "        self.max_len_sequence= max([len(proteins_seq[i]) for i in range(len(proteins_seq))])\n",
    "\n",
    "        #length of each sequence\n",
    "        self.lengths = torch.Tensor([len(prot) for prot in self.proteins_seq])\n",
    "\n",
    "        # Allow to import a vocabulary (for valid/test datasets, that will use the training vocabulary)\n",
    "        if vocab is not None:\n",
    "            self.aa2id, self.id2aa = vocab\n",
    "        else:\n",
    "            # If no vocabulary imported, build it (and reverse)\n",
    "            self.aa2id, self.id2aa = self.build_vocab()\n",
    "        self.vocab_size = len(self.aa2id)\n",
    "        #print(self.aa2id)\n",
    "        #print(self.id2aa)\n",
    "        #Convert to Tensor and apply the vocabulary\n",
    "        sequences_tensor = list(map(lambda prot: torch.Tensor([self.aa2id[aa] for aa in prot]),self.proteins_seq))\n",
    "        #Pad the sequence\n",
    "        sequences_padded = pad_sequence(sequences_tensor,batch_first = True, padding_value =0)\n",
    "\n",
    "        #One Hot encoding\n",
    "        sequences_encoded = one_hot(sequences_padded.to(torch.int64),num_classes=self.vocab_size)\n",
    "\n",
    "        #Convert to Torch \n",
    "        self.X = sequences_encoded\n",
    "        self.y = torch.Tensor(self.y)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.X,self.y\n",
    "    \n",
    "    def get_lengths(self):\n",
    "        return self.lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.proteins_seq)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The iterator just gets one particular example with its category\n",
    "        # The dataloader will take care of the shuffling and batching\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.X[idx], self.y[idx], self.lengths[idx]\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"\n",
    "          Function to build the Amino Acids vocabulary\n",
    "          \n",
    "          Returns\n",
    "          ----------------------------\n",
    "            aa2id : <dict{str:int}> \n",
    "                Dictionary to go from an Amino Acids to an id\n",
    "            id2aa : <dict{int:str}> \n",
    "                Dictionary to go from an id to an Amino Acids\n",
    "        \"\"\"\n",
    "        #Set of the Amino Acids\n",
    "        amino_acids = list(set(\"\".join(self.proteins_seq)))\n",
    "        #print(amino_acids)\n",
    "        #Build Vocab (+1 for the padding)\n",
    "        aa2id = {aa:i+1 for i, aa in enumerate(amino_acids)}\n",
    "        id2aa = {i+1:aa for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "        #Add the Padding id \n",
    "        aa2id = {\"PAD\":self.padding_mark,**aa2id}\n",
    "        id2aa = {**id2aa,self.padding_mark:\"PAD\"}\n",
    "\n",
    "        return aa2id, id2aa\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        # A simple way to get the training vocab when building the valid/test \n",
    "        return self.aa2id, self.id2aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinSeqdataset(train_x,train_y)\n",
    "val_dataset = ProteinSeqdataset(val_x,val_y)\n",
    "#X,y = train_dataset.get_data()\n",
    "aa2id,id2aa = train_dataset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 96., 369., 183.,  ...,  81., 779., 836.])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = train_dataset.get_lengths()\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_class = len(set(y_train))\n",
    "nb_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(aa2id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(train_dataset, batch_size = 200, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ProteinLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, vocab_size, embedding_dim):\n",
    "        super(ProteinLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        print(x.shape)\n",
    "        out, _ = self.lstm(x)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses \u001b[39m=\u001b[39m experiment(model, opt, criterion,num_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses = experiment(model, opt, criterion,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = ProteinLSTM(hidden_size=64, num_layers=2, num_classes=nb_class, vocab_size=vocab_size, embedding_dim=64)\n",
    "# Create an optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=0.0025, betas=(0.9, 0.999))\n",
    "# The criterion is a binary cross entropy loss based on logits - meaning that the sigmoid is integrated into the criterion\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def train_epoch(model, opt, criterion, dataloader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i, (x, y, len_) in enumerate(dataloader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        len_ = len_.to(device)\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        pred = model.forward(x, len_)\n",
    "        # (2) Compute the loss \n",
    "        loss = criterion(pred,y)\n",
    "        # (3) Compute gradients with the criterion\n",
    "        loss.backward()\n",
    "        # (4) Update weights with the optimizer\n",
    "        opt.step()    \n",
    "        losses.append(loss.item())\n",
    "        # Count the number of correct predictions in the batch - here, you'll need to use the sigmoid\n",
    "        num_corrects = (torch.round(torch.sigmoid(pred)) == y).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(y)\n",
    "        \n",
    "        if (i%20 == 0):\n",
    "            print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training acc = \" + str(acc.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the evaluation ! We don't need the optimizer here. \n",
    "def eval_model(model, criterion, evalloader):\n",
    "    model.eval()\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, len_) in enumerate(evalloader):\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            len_ = len_.to(device)\n",
    "            pred =  model.forward(x, len_)\n",
    "            loss = criterion(pred,y)\n",
    "            num_corrects =(torch.round(torch.sigmoid(pred)) == y).float().sum()\n",
    "            acc = 100.0 * num_corrects/len(y)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return total_epoch_loss/(i+1), total_epoch_acc/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function which will help you execute experiments rapidly - with a early_stopping option when necessary. \n",
    "def experiment(model, opt, criterion, num_epochs = 5, early_stopping = True):\n",
    "    train_losses = []\n",
    "    if early_stopping: \n",
    "        best_valid_loss = 10. \n",
    "    print(\"Beginning training...\")\n",
    "    for e in range(num_epochs):\n",
    "        print(\"Epoch \" + str(e+1) + \":\")\n",
    "        train_losses += train_epoch(model, opt, criterion, training_dataloader)\n",
    "        valid_loss, valid_acc = eval_model(model, criterion, valid_dataloader)\n",
    "        print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(valid_loss) + \"; Validation acc = \" + str(valid_acc))\n",
    "        if early_stopping:\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "            else:\n",
    "                print(\"Early stopping.\")\n",
    "                break  \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses \u001b[39m=\u001b[39m experiment(model, opt, criterion,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[235], line 9\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(model, opt, criterion, num_epochs, early_stopping)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     train_losses \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_epoch(model, opt, criterion, training_dataloader)\n\u001b[0;32m     10\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m eval_model(model, criterion, valid_dataloader)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m : Validation loss = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valid_loss) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m; Validation acc = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(valid_acc))\n",
      "Cell \u001b[1;32mIn[233], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, opt, criterion, dataloader)\u001b[0m\n\u001b[0;32m      8\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m \u001b[39m# (1) Forward\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(x, len_)\n\u001b[0;32m     11\u001b[0m \u001b[39m# (2) Compute the loss \u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred,y)\n",
      "Cell \u001b[1;32mIn[196], line 17\u001b[0m, in \u001b[0;36mProteinLSTM.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(x, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x)\n\u001b[0;32m     18\u001b[0m out, _ \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_packed_sequence(out, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 772\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[0;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:697\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    693\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[0;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    696\u001b[0m                        ):\n\u001b[1;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    699\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[0;32m    701\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hugo\\Documents\\Travail\\A5\\ALTEGRAD\\Challenge\\Altegrad\\.env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:206\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    204\u001b[0m expected_input_dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m3\u001b[39m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m expected_input_dim:\n\u001b[1;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 3"
     ]
    }
   ],
   "source": [
    "train_losses = experiment(model, opt, criterion,num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read sequences\n",
    "sequences = list()\n",
    "with open(\"data/sequences.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        sequences.append(line[:-1])\n",
    "\n",
    "# Split data into training and test sets\n",
    "sequences_train = list()\n",
    "sequences_test = list()\n",
    "proteins_test = list()\n",
    "y_train = list()\n",
    "with open(\"data/graph_labels.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        t = line.split(\",\")\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "            sequences_test.append(sequences[i])\n",
    "        else:\n",
    "            sequences_train.append(sequences[i])\n",
    "            y_train.append(int(t[1][:-1]))\n",
    "\n",
    "# Map sequences to\n",
    "vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1, 3))\n",
    "X_train = vec.fit_transform(sequences_train)\n",
    "X_test = vec.transform(sequences_test)\n",
    "\n",
    "# Train a logistic regression classifier and use the classifier to\n",
    "# make predictions\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba_seq = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class0</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>class4</th>\n",
       "      <th>class5</th>\n",
       "      <th>class6</th>\n",
       "      <th>class7</th>\n",
       "      <th>class8</th>\n",
       "      <th>class9</th>\n",
       "      <th>class10</th>\n",
       "      <th>class11</th>\n",
       "      <th>class12</th>\n",
       "      <th>class13</th>\n",
       "      <th>class14</th>\n",
       "      <th>class15</th>\n",
       "      <th>class16</th>\n",
       "      <th>class17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11as</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.010125</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.015560</td>\n",
       "      <td>0.110409</td>\n",
       "      <td>0.040535</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.055511</td>\n",
       "      <td>0.010177</td>\n",
       "      <td>0.009209</td>\n",
       "      <td>0.044630</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.082652</td>\n",
       "      <td>0.076744</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.009913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16pk</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.417447</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.013803</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.063944</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.088272</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.081549</td>\n",
       "      <td>0.008722</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>0.081467</td>\n",
       "      <td>0.069259</td>\n",
       "      <td>0.015457</td>\n",
       "      <td>0.007792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19hc</td>\n",
       "      <td>0.058075</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.224616</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.369082</td>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.122394</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.006151</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>0.044070</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ag9</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>0.456720</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.097420</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.097511</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.095374</td>\n",
       "      <td>0.054399</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>0.007580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1agx</td>\n",
       "      <td>0.104038</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>0.274372</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.019519</td>\n",
       "      <td>0.089925</td>\n",
       "      <td>0.012469</td>\n",
       "      <td>0.140387</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.034096</td>\n",
       "      <td>0.006613</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.174117</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.007112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name    class0    class1    class2    class3    class4    class5    class6  \\\n",
       "0  11as  0.013421  0.010125  0.466346  0.009600  0.015560  0.110409  0.040535   \n",
       "1  16pk  0.021742  0.016125  0.417447  0.009342  0.013803  0.056400  0.063944   \n",
       "2  19hc  0.058075  0.012037  0.224616  0.004999  0.011772  0.043618  0.369082   \n",
       "3  1ag9  0.017705  0.009405  0.456720  0.010811  0.011597  0.097420  0.034768   \n",
       "4  1agx  0.104038  0.013102  0.274372  0.023038  0.014659  0.019519  0.089925   \n",
       "\n",
       "     class7    class8    class9   class10   class11   class12   class13  \\\n",
       "0  0.015619  0.055511  0.010177  0.009209  0.044630  0.006132  0.005926   \n",
       "1  0.019685  0.088272  0.010683  0.008819  0.081549  0.008722  0.009492   \n",
       "2  0.011394  0.122394  0.008381  0.006151  0.020255  0.005745  0.009169   \n",
       "3  0.012282  0.097511  0.013075  0.012560  0.034495  0.007457  0.008537   \n",
       "4  0.012469  0.140387  0.014300  0.006727  0.034096  0.006613  0.019848   \n",
       "\n",
       "    class14   class15   class16   class17  \n",
       "0  0.082652  0.076744  0.017490  0.009913  \n",
       "1  0.081467  0.069259  0.015457  0.007792  \n",
       "2  0.030656  0.044070  0.012128  0.005458  \n",
       "3  0.095374  0.054399  0.018305  0.007580  \n",
       "4  0.174117  0.032383  0.013295  0.007112  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "seq = pd.read_csv(\"sample_submission.csv\")\n",
    "seq.head()\n",
    "#np.sum(seq,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = pd.read_csv(\"sample_submission.csv\")\n",
    "struct.head()\n",
    "#np.sum(struct,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [       nan, 0.01342103, 0.01012509, ..., 0.07674363, 0.0174905 ,\n",
       "        0.00991321],\n",
       "       [       nan, 0.02174239, 0.01612491, ..., 0.06925897, 0.01545664,\n",
       "        0.00779209],\n",
       "       ...,\n",
       "       [       nan, 0.02276303, 0.01242107, ..., 0.04877692, 0.01632783,\n",
       "        0.00632687],\n",
       "       [       nan, 0.02810728, 0.01194174, ..., 0.04407273, 0.01484368,\n",
       "        0.02052648],\n",
       "       [       nan, 0.13940033, 0.00669199, ..., 0.01033002, 0.00609641,\n",
       "        0.00639728]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data_seq = genfromtxt('sample_submission.csv', delimiter=',')\n",
    "my_data_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_seq,y_pred_proba_struct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "945fb5a7f3f5c89583143137abd22c2112b7db8dd1e354ffad5b7af59f8c26a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
